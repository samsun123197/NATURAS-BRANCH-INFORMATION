import requests
import os
import time
import random
import re 
from bs4 import BeautifulSoup 
from tqdm import tqdm 
from concurrent.futures import ThreadPoolExecutor, as_completed # Paralel iÅŸleme iÃ§in yeni modÃ¼ller

# --- AYARLANABÄ°LÄ°R PARAMETRELER ---

MAX_WORKERS = 10  # ðŸŒŸ EÅŸ zamanlÄ± Ã§alÄ±ÅŸacak iÅŸ parÃ§acÄ±ÄŸÄ± (istek) sayÄ±sÄ±. HÄ±z burada belirlenir.
DELAY_SECONDS = 2 # ðŸŒŸ Her bir iÅŸ parÃ§acÄ±ÄŸÄ± (worker) bekleme sÃ¼resini korur.
PROXY_FILE = "proxy.txt"
FAILED_ID_FILE = "failed_ids.txt" 
OUTPUT_TXT_FILE = "temiz_rapor_ozetleri.txt" 
# URL'ler ve HTTP ayarlarÄ± aynÄ± kalÄ±r...
# ... (URL_PREFIX, URL_SUFFIX, COOKIES, HEADERS tanÄ±mlarÄ± Ã¶nceki koddan alÄ±nmÄ±ÅŸtÄ±r)

# --- URL, COOKIES, HEADERS (Ã–nceki koddan kopyalayÄ±n) ---
URL_PREFIX = "https://vp.golfdondurma.com.tr/SAASReport/ReportView.aspx?CubeReportId=1013829&Values=2;(-1,"
URL_SUFFIX = ")|5;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;|;&RoleId=110628056449&LanguageId=1&UserId=114737182451&ReportTypeId=1&RW=1013832&CL=&DT=1163963292&Mode=ExportToText&Token=47d91344-f898-4996-80a7-788a4ec4f2d9"
COOKIES = {
    "_ga": "GA1.1.383992402.1742082046",
    "_ga_PESWVXLKNX": "GS1.1.1744028568.3.0.1744028568.60.0.0",
    "ASP.NET_SessionId": "fb1eohea3lhixihltffm0lrl"
}
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari537.36",
    "Referer": "https://vp.golfdondurma.com.tr/Report/reportcubeview2.aspx" 
}
# --- YARDIMCI VE LOGLAMA FONKSÄ°YONLARI ---

def load_proxies(file_path):
    # (Bu fonksiyon aynÄ± kalÄ±r, Ã¶nceki koddan kopyalayÄ±n)
    if not os.path.exists(file_path):
        return []
    with open(file_path, 'r') as f:
        proxies = [line.strip() for line in f if line.strip()]
    return proxies

def log_failed_id(id_value, reason, proxy=None):
    # (Bu fonksiyon aynÄ± kalÄ±r, Ã¶nceki koddan kopyalayÄ±n)
    proxy_info = f" | Proxy: {proxy}" if proxy else ""
    with open(FAILED_ID_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{id_value};{reason}{proxy_info}\n")

def get_user_inputs():
    # (Bu fonksiyon aynÄ± kalÄ±r, Ã¶nceki koddan kopyalayÄ±n)
    while True:
        try:
            start_id_str = input("LÃ¼tfen taramaya baÅŸlayacaÄŸÄ±nÄ±z Ä°LK ID numarasÄ±nÄ± girin (Ã–rn: 110618401108): ")
            start_id = int(start_id_str)
            
            count_str = input("LÃ¼tfen kaÃ§ adet ardÄ±ÅŸÄ±k ID taranacaÄŸÄ±nÄ± girin (Ã–rn: 100): ")
            count = int(count_str)
            
            if start_id < 0 or count <= 0:
                print("Hata: ID ve adet pozitif sayÄ± olmalÄ±dÄ±r.")
                continue
            
            return start_id, count
        except ValueError:
            print("Hata: LÃ¼tfen geÃ§erli bir sayÄ±sal deÄŸer girin.")
        except KeyboardInterrupt:
            print("\nÄ°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan iptal edildi.")
            exit()

# --- TEMEL Ä°Åž PARÃ‡ACIÄžI FONKSÄ°YONU ---

def process_id(current_id, proxies_list):
    """Tek bir ID'yi iÅŸleyen, istek gÃ¶nderen ve sonucu dÃ¶ndÃ¼ren fonksiyon."""
    
    full_url = f"{URL_PREFIX}{current_id}{URL_SUFFIX}"
    
    # Proxy seÃ§imi
    selected_proxy = None
    proxies_dict = None
    if proxies_list:
        selected_proxy = random.choice(proxies_list)
        proxies_dict = {
            "http": f"http://{selected_proxy}",
            "https": f"http://{selected_proxy}" 
        }

    try:
        response = requests.get(
            full_url, 
            headers=HEADERS, 
            cookies=COOKIES, 
            proxies=proxies_dict,
            timeout=15 
        )
        
        # Ä°stekler arasÄ±nda bekleme sÃ¼resi burada uygulanÄ±r.
        time.sleep(DELAY_SECONDS) 

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            distributor_tag = soup.find('td', class_='rowHeader0')
            
            cleaned_text = ""
            if distributor_tag:
                raw_text = distributor_tag.get_text()
                cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
            
            if cleaned_text:
                output_line = f"{current_id};{cleaned_text}\n"
                # BaÅŸarÄ±lÄ± sonuÃ§larÄ± dÃ¶ndÃ¼r
                return 'SUCCESS', output_line
            else:
                return 'FAILED', (current_id, "Ä°Ã§erik Etiketi BulunamadÄ± (Oturum Sorunu/GeÃ§ersiz ID)", selected_proxy)
        else:
            return 'FAILED', (current_id, f"HTTP HatasÄ±: {response.status_code}", selected_proxy)

    except requests.exceptions.RequestException as e:
        return 'FAILED', (current_id, f"BaÄŸlantÄ±/Proxy HatasÄ±: {str(e)[:50]}", selected_proxy)
    except Exception as e:
        return 'FAILED', (current_id, f"Genel Hata: {str(e)[:50]}", selected_proxy)


# --- ANA Ã‡OKLU Ä°ÅžLEME FONKSÄ°YONU ---

def run_mass_text_export_parallel():
    
    start_id, count = get_user_inputs()
    proxies_list = load_proxies(PROXY_FILE)
    
    if not proxies_list:
        print("âš ï¸ UyarÄ±: Proxy listesi boÅŸ. Ä°ÅŸlem tek IP Ã¼zerinden devam edecektir.")
        
    end_id = start_id + count - 1
    
    # Ã‡Ä±ktÄ± dosyalarÄ±nÄ± temizle
    if os.path.exists(OUTPUT_TXT_FILE): os.remove(OUTPUT_TXT_FILE)
    if os.path.exists(FAILED_ID_FILE): os.remove(FAILED_ID_FILE)
    
    print(f"\nID taramasÄ± {start_id}'den {end_id}'e kadar ({count} adet) {MAX_WORKERS} eÅŸ zamanlÄ± iÅŸ parÃ§acÄ±ÄŸÄ± ile baÅŸlayacaktÄ±r.")
    print("-" * 50)
    
    successful_exports = 0
    id_range = range(start_id, end_id + 1)
    
    # ThreadPoolExecutor kullanarak paralel Ã§alÄ±ÅŸtÄ±rma
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        
        # TÃ¼m ID'ler iÃ§in gÃ¶revleri gÃ¶nder
        future_to_id = {executor.submit(process_id, id_val, proxies_list): id_val for id_val in id_range}
        
        # tqdm ile ilerleme Ã§ubuÄŸunu gÃ¶ster
        results_iterator = tqdm(as_completed(future_to_id), total=count, desc="Paralel Tarama Ä°lerlemesi", unit="ID")

        for future in results_iterator:
            result_type, result_data = future.result()
            
            if result_type == 'SUCCESS':
                # BaÅŸarÄ±lÄ± sonuÃ§larÄ± ana dosyaya yazar
                with open(OUTPUT_TXT_FILE, 'a', encoding='utf-8') as f:
                    f.write(result_data)
                successful_exports += 1
                
            elif result_type == 'FAILED':
                # BaÅŸarÄ±sÄ±z sonuÃ§larÄ± log dosyasÄ±na yazar
                current_id, reason, selected_proxy = result_data
                log_failed_id(current_id, reason, selected_proxy)
                
            # Ä°lerleme Ã§ubuÄŸunun aÃ§Ä±klamasÄ±nÄ± gÃ¼ncelle
            results_iterator.set_postfix(BaÅŸarÄ±lÄ±=successful_exports, Hata=results_iterator.n - successful_exports)


    print("-" * 50)
    print(f"Ä°ÅŸlem tamamlandÄ±. Toplam baÅŸarÄ±lÄ± kayÄ±t: {successful_exports}")
    print(f"BaÅŸarÄ±sÄ±z ID'ler {FAILED_ID_FILE} dosyasÄ±na kaydedildi.")

if __name__ == "__main__":
    run_mass_text_export_parallel()